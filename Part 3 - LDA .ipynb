{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords\n",
    "from gensim import corpora, models, similarities\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('email_input.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>from</th>\n",
       "      <th>message-id</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>['Anyone', 'know', 'about', 'the', 'Weitek', '...</td>\n",
       "      <td>cavalier@blkbox.COM (Bill Egan)</td>\n",
       "      <td>&lt;1993Apr18.031714.3642@nntpxfer.psi.com&gt;</td>\n",
       "      <td>Weitek P9000 ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>['In', 'article', '1993Mar31', '140529', '1084...</td>\n",
       "      <td>arc@cco.caltech.edu (Aaron Ray Clements)</td>\n",
       "      <td>&lt;1pifnjINNscb@gap.caltech.edu&gt;</td>\n",
       "      <td>ACLU (was Re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>['and', 'I', 'm', 'sure', 'that', 'people', 'w...</td>\n",
       "      <td>hrubin@pop.stat.purdue.edu (Herman Rubin)</td>\n",
       "      <td>&lt;C5sFnz.Fo1@mentor.cc.purdue.edu&gt;</td>\n",
       "      <td>Gritz/JBS/Liberty Lobby/LaRouche/Christic Insi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>['In', 'article', '1r6p8oINN8hi', 'clem', 'han...</td>\n",
       "      <td>wwarf@silver.ucs.indiana.edu (Wayne J. Warf)</td>\n",
       "      <td>&lt;C5y2r9.4D7@usenet.ucs.indiana.edu&gt;</td>\n",
       "      <td>BD's did themselves--you're all paranoid freaks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>['I', 'recently', 'have', 'become', 'aware', '...</td>\n",
       "      <td>mon@cray.com (Muriel Nelson)</td>\n",
       "      <td>&lt;1993Apr15.154053.3087@hemlock.cray.com&gt;</td>\n",
       "      <td>ABORTION and private health coverage -- letter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0  ['Anyone', 'know', 'about', 'the', 'Weitek', '...   \n",
       "1  ['In', 'article', '1993Mar31', '140529', '1084...   \n",
       "2  ['and', 'I', 'm', 'sure', 'that', 'people', 'w...   \n",
       "3  ['In', 'article', '1r6p8oINN8hi', 'clem', 'han...   \n",
       "4  ['I', 'recently', 'have', 'become', 'aware', '...   \n",
       "\n",
       "                                           from  \\\n",
       "0               cavalier@blkbox.COM (Bill Egan)   \n",
       "1      arc@cco.caltech.edu (Aaron Ray Clements)   \n",
       "2     hrubin@pop.stat.purdue.edu (Herman Rubin)   \n",
       "3  wwarf@silver.ucs.indiana.edu (Wayne J. Warf)   \n",
       "4                  mon@cray.com (Muriel Nelson)   \n",
       "\n",
       "                                 message-id  \\\n",
       "0  <1993Apr18.031714.3642@nntpxfer.psi.com>   \n",
       "1            <1pifnjINNscb@gap.caltech.edu>   \n",
       "2         <C5sFnz.Fo1@mentor.cc.purdue.edu>   \n",
       "3       <C5y2r9.4D7@usenet.ucs.indiana.edu>   \n",
       "4  <1993Apr15.154053.3087@hemlock.cray.com>   \n",
       "\n",
       "                                             subject  \n",
       "0                                     Weitek P9000 ?  \n",
       "1                                       ACLU (was Re  \n",
       "2  Gritz/JBS/Liberty Lobby/LaRouche/Christic Insi...  \n",
       "3    BD's did themselves--you're all paranoid freaks  \n",
       "4  ABORTION and private health coverage -- letter...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LDA requires extra text-preprocessing. Custom functions are used to remove additional punctuation and words.\n",
    "These functions are then used to clean the body of the email messages to improve the accuracy in calculating the probability score assigned to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_and_clean(x):\n",
    "    x = x.strip('[')\n",
    "    x = x.strip(']')\n",
    "    x = x.replace(\"'\",\"\")\n",
    "    x = x.split(sep=',')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_words = []\n",
    "def extra_cleaning(x):\n",
    "    for n in range(len(x)):\n",
    "        if len(x[n]) < 2:\n",
    "            extra_words.append(x[n])\n",
    "        x[n] = x[n].strip()\n",
    "        x[n] = x[n].lower()\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = data['body'].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before Cleaning\n",
      "['Anyone', 'know', 'about', 'the', 'Weitek', 'P9000', 'graphics', 'chip', 'Do', 'you', 'have', 'Weitek', 's', 'address', 'phone', 'number', 'I', 'd', 'like', 'to', 'get', 'some', 'information', 'about', 'this', 'chip', 'Yes', 'I', 'am', 'very', 'interested', 'in', 'this', 'chip', 'Please', 'follow', 'up', 'or', 'email', 'Bill', 'EganCavalier', 'GraphicsHouston', 'Texas']\n"
     ]
    }
   ],
   "source": [
    "print(f'Before Cleaning')\n",
    "print(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CLEANING THE TOKENIZED TEXT USING CUSTOM FUNCTIONS\n",
    "for n in range(len(tokenized_text)):\n",
    "    tokenized_text[n] = extra_cleaning(split_and_clean(tokenized_text[n]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After Cleaning\n",
      "['anyone', 'know', 'about', 'the', 'weitek', 'p9000', 'graphics', 'chip', 'do', 'you', 'have', 'weitek', 's', 'address', 'phone', 'number', 'i', 'd', 'like', 'to', 'get', 'some', 'information', 'about', 'this', 'chip', 'yes', 'i', 'am', 'very', 'interested', 'in', 'this', 'chip', 'please', 'follow', 'up', 'or', 'email', 'bill', 'egancavalier', 'graphicshouston', 'texas']\n"
     ]
    }
   ],
   "source": [
    "print(f'After Cleaning')\n",
    "print(tokenized_text[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of stopwords removed from the dataset : 367\n"
     ]
    }
   ],
   "source": [
    "stopwords = ENGLISH_STOP_WORDS.union(['like', 'know', 'think', 'just','don','make','does','way','com','thanks'])\n",
    "add = ['0','1','2','3','4','5','6','7','8','9','_','yes','no']\n",
    "letters = list(string.ascii_lowercase)\n",
    "extra_words = extra_words + add + letters\n",
    "stopwords = stopwords.union(extra_words)\n",
    "print(f'Total number of stopwords removed from the dataset : {len(stopwords)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text Sample\n",
      "['weitek', 'p9000', 'graphics', 'chip', 'weitek', 'address', 'phone', 'number', 'information', 'chip', 'interested', 'chip', 'follow', 'email', 'egancavalier', 'graphicshouston', 'texas']\n"
     ]
    }
   ],
   "source": [
    "#Preparing the texts for LDA\n",
    "texts = [[word for word in text if word not in stopwords] for text in tokenized_text]\n",
    "print(f'Cleaned Text Sample')\n",
    "print(texts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, only the keywords from each email are extracted and stored in texts for 300 emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATING A DICTIONARY USING GENSIM\n",
    "dictionary = corpora.Dictionary(text for text in texts)\n",
    "\n",
    "#CREATING UPPER BOUND AND LOWER BOUND TO EXCLUDE UNNECESSARY WORDS\n",
    "dictionary.filter_extremes(no_below=1, no_above=0.8)\n",
    "\n",
    "#CREATING A BAG OF WORDS MODEL\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LDA MODEL READY\n"
     ]
    }
   ],
   "source": [
    "#BUILD A LDA MODEL FROM GENSIM\n",
    "LDA = models.LdaModel(corpus, num_topics=5, id2word=dictionary, update_every=5,chunksize=10000, passes=100)\n",
    "print('LDA MODEL READY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.004*\"people\" + 0.004*\"government\" + 0.003*\"said\" + 0.003*\"want\" + 0.003*\"believe\" + 0.003*\"power\" + 0.003*\"pay\" + 0.002*\"police\" + 0.002*\"state\" + 0.002*\"league\"'),\n",
       " (1,\n",
       "  '0.005*\"data\" + 0.005*\"image\" + 0.005*\"edu\" + 0.003*\"available\" + 0.003*\"ftp\" + 0.002*\"use\" + 0.002*\"graphics\" + 0.002*\"software\" + 0.002*\"package\" + 0.002*\"images\"'),\n",
       " (2,\n",
       "  '0.007*\"people\" + 0.006*\"gun\" + 0.005*\"government\" + 0.004*\"edu\" + 0.003*\"right\" + 0.003*\"fbi\" + 0.003*\"guns\" + 0.003*\"batf\" + 0.003*\"time\" + 0.002*\"children\"'),\n",
       " (3,\n",
       "  '0.011*\"jpeg\" + 0.011*\"image\" + 0.005*\"file\" + 0.005*\"gif\" + 0.004*\"images\" + 0.004*\"color\" + 0.004*\"format\" + 0.003*\"edu\" + 0.003*\"president\" + 0.003*\"software\"'),\n",
       " (4,\n",
       "  '0.003*\"problem\" + 0.003*\"image\" + 0.003*\"otis\" + 0.003*\"gun\" + 0.003*\"lines\" + 0.002*\"homosexual\" + 0.002*\"files\" + 0.002*\"use\" + 0.002*\"pex\" + 0.002*\"edu\"')]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LDA.show_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Words for Cluster 1 with Probability Score\n",
      "[\"('people', 0.0038745147)\", \"('government', 0.0035155138)\", \"('said', 0.0029258046)\", \"('want', 0.0027021258)\", \"('believe', 0.0026552991)\", \"('power', 0.002619269)\", \"('pay', 0.0025032319)\", \"('police', 0.0022943616)\", \"('state', 0.0022623087)\", \"('league', 0.0022029597)\", \"('right', 0.0020094109)\", \"('fbi', 0.0019954387)\", \"('defamation', 0.0018912028)\", \"('cement', 0.0018912028)\", \"('did', 0.0018464912)\", \"('really', 0.0018237222)\", \"('anti', 0.0017875676)\", \"('information', 0.0017655236)\", \"('rights', 0.001753033)\", \"('use', 0.0016902918)\"]\n",
      "\n",
      "\n",
      "Top Words for Cluster 2 with Probability Score\n",
      "[\"('data', 0.005244993)\", \"('image', 0.0046453793)\", \"('edu', 0.0045123477)\", \"('available', 0.0029852265)\", \"('ftp', 0.002595399)\", \"('use', 0.0023694346)\", \"('graphics', 0.002285223)\", \"('software', 0.0021593934)\", \"('package', 0.0020160535)\", \"('images', 0.00195763)\", \"('line', 0.0017359726)\", \"('insurance', 0.0016314664)\", \"('program', 0.0016029011)\", \"('people', 0.0015876103)\", \"('vertex', 0.0015542601)\", \"('3d', 0.0015399666)\", \"('time', 0.0015272284)\", \"('display', 0.0015037721)\", \"('pub', 0.0014773889)\", \"('look', 0.0014367885)\"]\n",
      "\n",
      "\n",
      "Top Words for Cluster 3 with Probability Score\n",
      "[\"('people', 0.0072553144)\", \"('gun', 0.005960904)\", \"('government', 0.004503462)\", \"('edu', 0.0035121732)\", \"('right', 0.0031173432)\", \"('fbi', 0.0030824952)\", \"('guns', 0.0030053293)\", \"('batf', 0.002848202)\", \"('time', 0.0025757458)\", \"('children', 0.0024721131)\", \"('did', 0.0023396078)\", \"('ve', 0.0021644311)\", \"('believe', 0.0020842876)\", \"('law', 0.0020738621)\", \"('person', 0.0020013826)\", \"('koresh', 0.0019868922)\", \"('clinton', 0.0019850822)\", \"('said', 0.001898108)\", \"('good', 0.0018441826)\", \"('say', 0.0017673937)\"]\n",
      "\n",
      "\n",
      "Top Words for Cluster 4 with Probability Score\n",
      "[\"('jpeg', 0.0109847)\", \"('image', 0.0105079245)\", \"('file', 0.0049561257)\", \"('gif', 0.0047572986)\", \"('images', 0.0043249093)\", \"('color', 0.004140815)\", \"('format', 0.0038086982)\", \"('edu', 0.0032666537)\", \"('president', 0.0032167183)\", \"('software', 0.0030897192)\", \"('use', 0.0030725247)\", \"('version', 0.0030721172)\", \"('data', 0.0030347917)\", \"('bit', 0.0030046343)\", \"('files', 0.0028832564)\", \"('free', 0.0027493425)\", \"('did', 0.002564036)\", \"('available', 0.002555312)\", \"('people', 0.0025157225)\", \"('quality', 0.002307675)\"]\n",
      "\n",
      "\n",
      "Top Words for Cluster 5 with Probability Score\n",
      "[\"('problem', 0.0032063397)\", \"('image', 0.0032051909)\", \"('otis', 0.003067358)\", \"('gun', 0.002553501)\", \"('lines', 0.0025028016)\", \"('homosexual', 0.0023650022)\", \"('files', 0.0023096302)\", \"('use', 0.0022387134)\", \"('pex', 0.0022383137)\", \"('edu', 0.0022029572)\", \"('people', 0.0020001414)\", \"('view', 0.0019622522)\", \"('men', 0.0019266541)\", \"('table', 0.0018148177)\", \"('public', 0.0018031406)\", \"('line', 0.001686064)\", \"('areas', 0.0016858466)\", \"('new', 0.0016572586)\", \"('send', 0.0016341117)\", \"('time', 0.0015778475)\"]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#CREATE A NUMPY ARRAY OF TOPICS\n",
    "topics = LDA.show_topics(formatted=False, num_words=20)\n",
    "topics = np.array(topics)\n",
    "\n",
    "cluster_id = 1\n",
    "for n in topics[:,:]:\n",
    "    print(f'Top Words for Cluster {cluster_id} with Probability Score')\n",
    "    print([str(word) for word in n[1]])\n",
    "    print('\\n')\n",
    "    cluster_id += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
