{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing all necessary libraries for parsing, cleaning and basic pre-processing\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import string\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the email files are present in a directory/folder called data. To understand how to clean these files, the contents of the emails must be analysed. A single email is read to display the contents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xref: cantaloupe.srv.cs.cmu.edu talk.politics.misc:176869 alt.sex:110443 soc.men:67802 misc.legal:59959\n",
      "\n",
      "Path: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!husc-news.harvard.edu!hsdndev!yale!gumby!wupost!zaphod.mps.ohio-state.edu!ub!galileo.cc.rochester.edu!uhura.cc.rochester.edu!as010b\n",
      "\n",
      "Newsgroups: talk.politics.misc,alt.sex,soc.men,misc.legal\n",
      "\n",
      "Subject: Re: Stop putting down white het males.\n",
      "\n",
      "Message-ID: <1993Apr5.213327.23802@galileo.cc.rochester.edu>\n",
      "\n",
      "From: as010b@uhura.cc.rochester.edu (Tree of Schnopia)\n",
      "\n",
      "Date: 5 Apr 93 21:33:27 GMT\n",
      "\n",
      "Sender: news@galileo.cc.rochester.edu\n",
      "\n",
      "References: <DJNg2B1w165w@cybernet.cse.fau.edu> <C50FHG.MEA@ocsmd.ocs.com>\n",
      "\n",
      "Organization: University of Rochester - Rochester, New York\n",
      "\n",
      "Nntp-Posting-Host: uhura.cc.rochester.edu\n",
      "\n",
      "Lines: 32\n",
      "\n",
      "\n",
      "\n",
      "In <C50FHG.MEA@ocsmd.ocs.com> mark@ocsmd.ocs.com (Mark Wilson) writes:\n",
      "\n",
      "\n",
      "\n",
      ">Yuri Villanueva (elmo@cybernet.cse.fau.edu) wrote:\n",
      "\n",
      ">: pbray@envy.reed.edu (Public account) writes:\n",
      "\n",
      ">: \n",
      "\n",
      ">: > In article <1993Apr2.180839.14305@galileo.cc.rochester.edu>  \n",
      "\n",
      ">: > as010b@uhura.cc.rochester.edu (Tree of Schnopia) writes:\n",
      "\n",
      ">: >> In <1993Apr2.064804.29008@jato.jpl.nasa.gov>  \n",
      "\n",
      ">: >> michael@neuron6.jpl.nasa.gov (Michael Rivero) writes:\n",
      "\n",
      ">: >> \n",
      "\n",
      ">: >>We are told, by U.S. congresswoman Barbara Jordan, that we are biologically\n",
      "\n",
      ">: >>incapable of compassion.\n",
      "\n",
      ">Personally, I doubt she said anything of the kind, but if\n",
      "\n",
      ">someone can provide the ORIGINAL quote, IN CONTEXT, WITH SOURCE\n",
      "\n",
      ">(for, ahem, cross-checking), I would we willing to agree\n",
      "\n",
      ">she is full of sh*t.  Naturally, if no one can provide these\n",
      "\n",
      ">bits of data, the paraphrase listed must be disregarded,\n",
      "\n",
      ">and its poster regarded as full of sh*t.  OK, so which will it be?\n",
      "\n",
      "\n",
      "\n",
      "I followed up without a thought of double-checking...if I double-checked\n",
      "\n",
      "every fact people vomited onto the table here on the net, I'd never have\n",
      "\n",
      "time to sleep.  But to pass the buck to the person who originally posted\n",
      "\n",
      "that quote...\n",
      "\n",
      "\n",
      "\n",
      "...well, Michael?  Take it away!  (wild applause)\n",
      "\n",
      "\n",
      "\n",
      "Drewcifer\n",
      "\n",
      "-- \n",
      "\n",
      "----bi    Andrew D. Simchik\t\t\t\t\tSCHNOPIA!\n",
      "\n",
      "\\ ----    as010b@uhura.cc.rochester.edu\t\t\t\tTreeWater\n",
      "\n",
      " \\\\  /    \n",
      "\n",
      "   \\/     \"Words Weren't Made For Cowards\"--Happy Rhodes\n",
      "\n"
     ]
    }
   ],
   "source": [
    "file_path = \"data/176869\"\n",
    "file = open(file_path, encoding=\"utf8\", errors='ignore')\n",
    "contents = file.readlines()\n",
    "for content in contents:\n",
    "    print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every email has a similar format. Of which, a lot of the content is unncessary for the objective of this task.\n",
    "Thus, I extract only subject of the email, the body of the email, the from/sender details, and message-ID. \n",
    "\n",
    "A function to parse each email and extract only these contents is implemented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_files(raw_message):\n",
    "    \n",
    "    #split the raw_message into lines using delimiter\n",
    "    lines = raw_message.split('\\n')\n",
    "    \n",
    "    message = ''\n",
    "    \n",
    "    context = {}\n",
    "    \n",
    "    #extract only from, subject and message-id\n",
    "    keywords = ['from', 'subject','message-id']\n",
    "    \n",
    "    #in every line if : is not present then it is the message body and can be extracted\n",
    "    for line in lines:\n",
    "        \n",
    "        if ':' not in line:\n",
    "            message += line.strip()\n",
    "            context['body'] = message\n",
    "            \n",
    "        #the rest is xref, subject, from, to, id, time, etc \n",
    "        #we extract only what we need and put it in the email dict\n",
    "        else:\n",
    "            pairs = line.split(':')\n",
    "            key = pairs[0].lower()\n",
    "            value = pairs[1].strip()\n",
    "            \n",
    "            #code to extract subject contents when Re: is used \n",
    "            if str(key) == 'subject':\n",
    "                if len(pairs) > 2:\n",
    "                    val2 = pairs[2].strip()\n",
    "                    value = val2\n",
    "            \n",
    "            #extract the rest of the key value pairs\n",
    "            if key in keywords:\n",
    "                context[key] = value\n",
    "                \n",
    "    #return the email context dictionary\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Xref: cantaloupe.srv.cs.cmu.edu talk.politics.misc:176869 alt.sex:110443 soc.men:67802 misc.legal:59959\\nPath: cantaloupe.srv.cs.cmu.edu!das-news.harvard.edu!husc-news.harvard.edu!hsdndev!yale!gumby!wupost!zaphod.mps.ohio-state.edu!ub!galileo.cc.rochester.edu!uhura.cc.rochester.edu!as010b\\nNewsgroups: talk.politics.misc,alt.sex,soc.men,misc.legal\\nSubject: Re: Stop putting down white het males.\\nMessage-ID: <1993Apr5.213327.23802@galileo.cc.rochester.edu>\\nFrom: as010b@uhura.cc.rochester.edu (Tree of Schnopia)\\nDate: 5 Apr 93 21:33:27 GMT\\nSender: news@galileo.cc.rochester.edu\\nReferences: <DJNg2B1w165w@cybernet.cse.fau.edu> <C50FHG.MEA@ocsmd.ocs.com>\\nOrganization: University of Rochester - Rochester, New York\\nNntp-Posting-Host: uhura.cc.rochester.edu\\nLines: 32\\n\\nIn <C50FHG.MEA@ocsmd.ocs.com> mark@ocsmd.ocs.com (Mark Wilson) writes:\\n\\n>Yuri Villanueva (elmo@cybernet.cse.fau.edu) wrote:\\n>: pbray@envy.reed.edu (Public account) writes:\\n>: \\n>: > In article <1993Apr2.180839.14305@galileo.cc.rochester.edu>  \\n>: > as010b@uhura.cc.rochester.edu (Tree of Schnopia) writes:\\n>: >> In <1993Apr2.064804.29008@jato.jpl.nasa.gov>  \\n>: >> michael@neuron6.jpl.nasa.gov (Michael Rivero) writes:\\n>: >> \\n>: >>We are told, by U.S. congresswoman Barbara Jordan, that we are biologically\\n>: >>incapable of compassion.\\n>Personally, I doubt she said anything of the kind, but if\\n>someone can provide the ORIGINAL quote, IN CONTEXT, WITH SOURCE\\n>(for, ahem, cross-checking), I would we willing to agree\\n>she is full of sh*t.  Naturally, if no one can provide these\\n>bits of data, the paraphrase listed must be disregarded,\\n>and its poster regarded as full of sh*t.  OK, so which will it be?\\n\\nI followed up without a thought of double-checking...if I double-checked\\nevery fact people vomited onto the table here on the net, I\\'d never have\\ntime to sleep.  But to pass the buck to the person who originally posted\\nthat quote...\\n\\n...well, Michael?  Take it away!  (wild applause)\\n\\nDrewcifer\\n-- \\n----bi    Andrew D. Simchik\\t\\t\\t\\t\\tSCHNOPIA!\\n\\\\ ----    as010b@uhura.cc.rochester.edu\\t\\t\\t\\tTreeWater\\n \\\\\\\\  /    \\n   \\\\/     \"Words Weren\\'t Made For Cowards\"--Happy Rhodes\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#join the contents and store it in a string\n",
    "raw = ''.join(str(content) for content in contents)\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subject': 'Stop putting down white het males.',\n",
       " 'message-id': '<1993Apr5.213327.23802@galileo.cc.rochester.edu>',\n",
       " 'from': 'as010b@uhura.cc.rochester.edu (Tree of Schnopia)',\n",
       " 'body': '>Personally, I doubt she said anything of the kind, but if>someone can provide the ORIGINAL quote, IN CONTEXT, WITH SOURCE>(for, ahem, cross-checking), I would we willing to agree>she is full of sh*t.  Naturally, if no one can provide these>bits of data, the paraphrase listed must be disregarded,>and its poster regarded as full of sh*t.  OK, so which will it be?I followed up without a thought of double-checking...if I double-checkedevery fact people vomited onto the table here on the net, I\\'d never havetime to sleep.  But to pass the buck to the person who originally postedthat quote......well, Michael?  Take it away!  (wild applause)Drewcifer------bi    Andrew D. Simchik\\t\\t\\t\\t\\tSCHNOPIA!\\\\ ----    as010b@uhura.cc.rochester.edu\\t\\t\\t\\tTreeWater\\\\\\\\  /\\\\/     \"Words Weren\\'t Made For Cowards\"--Happy Rhodes'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse_files(raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function parse_files() is working and can be implemented for all the emails in the data directory.\n",
    "\n",
    "All the files in the data directory can be read using glob and parsed using the function above. The parsed content is then stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "#create an empty pandas dataframe\n",
    "main = pd.DataFrame()\n",
    "print(main)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files Read : 300\n",
      "Dataframe Processed\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#READ THE FILES\n",
    "#PARSE FILES\n",
    "#APPEND TO DATAFRAME\n",
    "\n",
    "count = 0\n",
    "for file in glob.glob('data/*'):\n",
    "    #print(file)\n",
    "    f = open(file, encoding=\"utf8\", errors='ignore')\n",
    "    data = f.readlines()\n",
    "    raw = ''.join(str(n) for n in data)\n",
    "    parsed = parse_files(raw)\n",
    "    #print(parsed)\n",
    "    main = main.append(parsed,ignore_index=True)\n",
    "    count += 1\n",
    "    #break\n",
    "    \n",
    "print(f'Files Read : {count}')\n",
    "print(f'Dataframe Processed\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 300 entries, 0 to 299\n",
      "Data columns (total 4 columns):\n",
      "body          300 non-null object\n",
      "from          300 non-null object\n",
      "message-id    300 non-null object\n",
      "subject       300 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 9.5+ KB\n"
     ]
    }
   ],
   "source": [
    "main.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 300 entries, 0 to 299\n",
      "Data columns (total 4 columns):\n",
      "body          300 non-null object\n",
      "from          300 non-null object\n",
      "message-id    300 non-null object\n",
      "subject       300 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 11.7+ KB\n"
     ]
    }
   ],
   "source": [
    "#DROP NULL COLUMNS IF ANY\n",
    "main = main.dropna()\n",
    "main.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No data is lost in the process. All 300 emails are read, parsed, and stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is preprocessing the dataframe contents. To group emails into similar categories, we would have to use an unsupervised learning approach - clustering. To achieve this, the body of each email has to be split, tokenized, and the punctuations must be removed, so the words can be vectorized and analyzed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried cleaning the contents of the email body using a custom function and with the NLTK preprocessing and tokenizing libraries. NLTK had better results and higher accuracy. Thus NLTK libaries with regular expression\n",
    "matching was used to tokenize the email body for every file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nltk(text):\n",
    "    \n",
    "    #Tokenize and remove punctutation using RegExp Tokenizer from NLTK\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    \n",
    "    #tokenize text\n",
    "    text = tokenizer.tokenize(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#APPLY THE ABOVE CLEAN_NLTK FUNCTION TO THE PANDAS DATAFRAME\n",
    "main['body'] = main['body'].apply(clean_nltk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body</th>\n",
       "      <th>from</th>\n",
       "      <th>message-id</th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>[Anyone, know, about, the, Weitek, P9000, grap...</td>\n",
       "      <td>cavalier@blkbox.COM (Bill Egan)</td>\n",
       "      <td>&lt;1993Apr18.031714.3642@nntpxfer.psi.com&gt;</td>\n",
       "      <td>Weitek P9000 ?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>[In, article, 1993Mar31, 140529, 10843, news, ...</td>\n",
       "      <td>arc@cco.caltech.edu (Aaron Ray Clements)</td>\n",
       "      <td>&lt;1pifnjINNscb@gap.caltech.edu&gt;</td>\n",
       "      <td>ACLU (was Re</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>[and, I, m, sure, that, people, who, were, big...</td>\n",
       "      <td>hrubin@pop.stat.purdue.edu (Herman Rubin)</td>\n",
       "      <td>&lt;C5sFnz.Fo1@mentor.cc.purdue.edu&gt;</td>\n",
       "      <td>Gritz/JBS/Liberty Lobby/LaRouche/Christic Insi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>[In, article, 1r6p8oINN8hi, clem, handheld, co...</td>\n",
       "      <td>wwarf@silver.ucs.indiana.edu (Wayne J. Warf)</td>\n",
       "      <td>&lt;C5y2r9.4D7@usenet.ucs.indiana.edu&gt;</td>\n",
       "      <td>BD's did themselves--you're all paranoid freaks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>[I, recently, have, become, aware, that, my, h...</td>\n",
       "      <td>mon@cray.com (Muriel Nelson)</td>\n",
       "      <td>&lt;1993Apr15.154053.3087@hemlock.cray.com&gt;</td>\n",
       "      <td>ABORTION and private health coverage -- letter...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                body  \\\n",
       "0  [Anyone, know, about, the, Weitek, P9000, grap...   \n",
       "1  [In, article, 1993Mar31, 140529, 10843, news, ...   \n",
       "2  [and, I, m, sure, that, people, who, were, big...   \n",
       "3  [In, article, 1r6p8oINN8hi, clem, handheld, co...   \n",
       "4  [I, recently, have, become, aware, that, my, h...   \n",
       "\n",
       "                                           from  \\\n",
       "0               cavalier@blkbox.COM (Bill Egan)   \n",
       "1      arc@cco.caltech.edu (Aaron Ray Clements)   \n",
       "2     hrubin@pop.stat.purdue.edu (Herman Rubin)   \n",
       "3  wwarf@silver.ucs.indiana.edu (Wayne J. Warf)   \n",
       "4                  mon@cray.com (Muriel Nelson)   \n",
       "\n",
       "                                 message-id  \\\n",
       "0  <1993Apr18.031714.3642@nntpxfer.psi.com>   \n",
       "1            <1pifnjINNscb@gap.caltech.edu>   \n",
       "2         <C5sFnz.Fo1@mentor.cc.purdue.edu>   \n",
       "3       <C5y2r9.4D7@usenet.ucs.indiana.edu>   \n",
       "4  <1993Apr15.154053.3087@hemlock.cray.com>   \n",
       "\n",
       "                                             subject  \n",
       "0                                     Weitek P9000 ?  \n",
       "1                                       ACLU (was Re  \n",
       "2  Gritz/JBS/Liberty Lobby/LaRouche/Christic Insi...  \n",
       "3    BD's did themselves--you're all paranoid freaks  \n",
       "4  ABORTION and private health coverage -- letter...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#now the body is cleaned and tokenized\n",
    "main.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "Look for email_input.csv in the current directory\n"
     ]
    }
   ],
   "source": [
    "#THE FINAL DATAFRAME IS THEN EXPORTED AS A CSV FILE FOR CLUSTERING AND LDA SO THE ABOVE STEPS NEED NOT BE \n",
    "#REPEATED\n",
    "print(main.to_csv('email_input.csv',index=False))\n",
    "print(f'Look for email_input.csv in the current directory')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
